{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32121d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sachinthaka/work/canva/tools/build/python/third_party/.venv/lib/python3.11/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "import peft\n",
    "import pydantic\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "from transformers import Trainer, TrainerCallback, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7dd1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "MASK_TOKEN = \"<|mask|>\"\n",
    "IM_START_TOKEN = \"<|im_start|>\"\n",
    "\n",
    "device: torch.device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "\n",
    "class LoraConfig(pydantic.BaseModel):\n",
    "    r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: list[str] = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ]\n",
    "    bias: str = \"none\"\n",
    "\n",
    "\n",
    "class TrainerConfig(pydantic.BaseModel):\n",
    "    model: str = MODEL\n",
    "    lr: float = 2e-4\n",
    "    epochs: int = 1\n",
    "    batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    log_interval: int = 1000\n",
    "    lora_config: LoraConfig = LoraConfig()\n",
    "\n",
    "\n",
    "trainer_config = TrainerConfig()\n",
    "\n",
    "\n",
    "# _ = wandb.init(\n",
    "#     project=\"diffusion-llm\",\n",
    "#     entity=\"sachinruk\",\n",
    "#     name=datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "#     config=trainer_config.model_dump(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4341a47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297981f909fc44a8b3d3d592dfa4211d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(151670, 2560)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model: transformers.modeling_utils.PreTrainedModel = (\n",
    "    transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=trainer_config.model,\n",
    "        torch_dtype=torch.bfloat16 if device.type in {\"cuda\", \"mps\"} else torch.float32,\n",
    "        attn_implementation=\"flash_attention_2\" if device.type == \"cuda\" else \"sdpa\",\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    ")\n",
    "tokenizer: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    trainer_config.model\n",
    ")\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [MASK_TOKEN]})\n",
    "llm_model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37362545",
   "metadata": {},
   "source": [
    "## Converting to Universal Attention\n",
    "\n",
    "To replace causal attention with universal (bidirectional) attention, we need to modify the attention mechanism. The key difference is that causal attention prevents tokens from attending to future positions, while universal attention allows all positions to attend to all other positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740f9d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(torch.nn.functional.scaled_dot_product_attention, \"_is_patched\"):\n",
    "    original_sdpa = torch.nn.functional.scaled_dot_product_attention\n",
    "\n",
    "    def universal_sdpa(\n",
    "        query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None\n",
    "    ):\n",
    "        if attn_mask is not None:\n",
    "            last_row = attn_mask[..., -1, :]\n",
    "            universal_mask = last_row.unsqueeze(-2).expand_as(attn_mask)\n",
    "            attn_mask = universal_mask\n",
    "        # ... your code ...\n",
    "        return original_sdpa(\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            attn_mask=attn_mask,\n",
    "            dropout_p=dropout_p,\n",
    "            is_causal=False,\n",
    "            scale=scale,\n",
    "        )\n",
    "\n",
    "    universal_sdpa._is_patched = True\n",
    "    torch.nn.functional.scaled_dot_product_attention = universal_sdpa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b21dfbb",
   "metadata": {},
   "source": [
    "### Testing Universal Attention\n",
    "\n",
    "Let's verify that the model now uses bidirectional attention:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7f96891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample input\n",
    "\n",
    "test_text = [\"The cat sat on the mat.\", \"The dog\"]\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "# Forward pass with universal attention\n",
    "with torch.no_grad():\n",
    "    outputs = llm_model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12cdc463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.CausalLMOutputWithPast"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a1d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = datasets.load_dataset(\n",
    "    \"allenai/tulu-3-sft-mixture-0225\",\n",
    "    split=\"train\",           # [:1%] % for demo.  drop the slice for real training\n",
    "    cache_dir=\"./data\",\n",
    "    download_mode=datasets.DownloadMode.REUSE_DATASET_IF_EXISTS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7b26d6",
   "metadata": {},
   "source": [
    "## Train for Next Token Prediction\n",
    "\n",
    "This section implements full next token prediction training (not just on assistant responses) using:\n",
    "- **LoRA (Low-Rank Adaptation)**: Efficient fine-tuning by only training low-rank adapter matrices\n",
    "- **Wandb Integration**: Logging losses and sample outputs\n",
    "- **Batch Size 16**: With gradient accumulation for stability\n",
    "- **Sample Logging**: Every 1000 iterations, we log input, expected output, and actual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cafb04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_input_ids(\n",
    "    input_ids: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    mask_probability: float,\n",
    "    mask_token_id: int,\n",
    "    response_mask: torch.Tensor | None = None,\n",
    ") -> torch.Tensor:\n",
    "    mask_indices = torch.bernoulli(\n",
    "        torch.full(size=input_ids.shape, fill_value=mask_probability, device=input_ids.device)\n",
    "    ).bool()\n",
    "    mask_indices = mask_indices & attention_mask.bool()\n",
    "    if response_mask is not None:\n",
    "        mask_indices: torch.Tensor = mask_indices & response_mask\n",
    "    input_ids[mask_indices] = mask_token_id\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def get_prefix_mask(input_ids: torch.Tensor, separator_token_id: int) -> torch.Tensor:\n",
    "    \"\"\"Mask labels before the last separator token.\n",
    "\n",
    "    Args:\n",
    "        labels: The labels to mask.\n",
    "        separator_token_id: The ID of the separator token.\n",
    "        ignore_index: The index to use for the ignored labels.\n",
    "\n",
    "    Returns:\n",
    "        The masked labels.\n",
    "    \"\"\"\n",
    "    sep_mask = input_ids == separator_token_id  # (B, L) booleans\n",
    "    s = sep_mask.cumsum(dim=1)  # running count of seps\n",
    "    total = s[:, -1:]  # total seps per row (B, 1)\n",
    "    last_sep_onehot = sep_mask & (s == total)  # 1 only at the last sep (or all 0 if none)\n",
    "\n",
    "    # Build a mask of positions <= last separator (inclusive):\n",
    "    # reverse -> cumsum -> reverse gives ones from start up to that last-sep index\n",
    "    prefix_mask = last_sep_onehot.flip(dims=[1]).cumsum(dim=1).flip(dims=[1]).bool()\n",
    "    return prefix_mask\n",
    "\n",
    "\n",
    "def tokenize_text(\n",
    "    examples: list[dict[str, Any]],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    max_length: int = 1024,\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    string_examples: list[str] = tokenizer.apply_chat_template(\n",
    "        [example[\"messages\"] for example in examples],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "\n",
    "    return tokenizer(\n",
    "        string_examples,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "class CollateFn:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        max_length: int = 1024,\n",
    "        min_mask_probability: float = 0.1,\n",
    "        max_mask_probability: float = 0.95,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_token_id: int = tokenizer.convert_tokens_to_ids(MASK_TOKEN)\n",
    "        self.sep_token_id: int = tokenizer.convert_tokens_to_ids(IM_START_TOKEN)\n",
    "        self.ignore_index = -100\n",
    "        self.max_length = max_length\n",
    "        self.min_mask_probability = min_mask_probability\n",
    "        self.max_mask_probability = max_mask_probability\n",
    "\n",
    "\n",
    "class PretrainingCollateFn(CollateFn):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, examples: list[dict[str, Any]]) -> dict[str, torch.Tensor]:\n",
    "        encoded_batch = tokenize_text(examples, self.tokenizer, self.max_length)\n",
    "        input_ids = encoded_batch[\"input_ids\"]\n",
    "        attn = encoded_batch[\"attention_mask\"]\n",
    "\n",
    "        labels: torch.Tensor = input_ids.clone()\n",
    "        labels[attn == 0] = self.ignore_index\n",
    "\n",
    "        input_ids = mask_input_ids(\n",
    "            input_ids,\n",
    "            attn,\n",
    "            mask_probability=random.uniform(self.min_mask_probability, self.max_mask_probability),\n",
    "            mask_token_id=self.mask_token_id,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attn,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "\n",
    "class SFTCollateFn(CollateFn):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, examples: list[dict[str, Any]]) -> dict[str, torch.Tensor]:\n",
    "        encoded_batch = tokenize_text(examples, self.tokenizer, self.max_length)\n",
    "\n",
    "        input_ids = encoded_batch[\"input_ids\"]\n",
    "        attn = encoded_batch[\"attention_mask\"]\n",
    "        prefix_mask = get_prefix_mask(input_ids, self.sep_token_id)\n",
    "        labels: torch.Tensor = input_ids.clone()\n",
    "        labels[attn == 0] = self.ignore_index\n",
    "        labels[prefix_mask] = self.ignore_index\n",
    "\n",
    "        input_ids = mask_input_ids(\n",
    "            input_ids,\n",
    "            attn,\n",
    "            mask_probability=random.uniform(self.min_mask_probability, self.max_mask_probability),\n",
    "            mask_token_id=self.mask_token_id,\n",
    "            response_mask=~prefix_mask,\n",
    "        )\n",
    "\n",
    "        encoded_batch[\"labels\"] = labels\n",
    "        return encoded_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5dd7b5",
   "metadata": {},
   "source": [
    "Test out CollateFns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13c416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Create a snippet of Terraform HCL code that create an AWS autoscaling group, and an ALB in front to expose an application to internet.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Sure, here's an example Terraform H<|mask|> code that creates an AWS Autoscaling Group<|mask|><|mask|><|mask|> Load Balancer to<|mask|> an<|mask|> to the internet:\n",
      "``` \n",
      "# Configure<|mask|><|mask|> provider<|mask|>provider \"aws\" {\n",
      "  region = \"<|mask|><|mask|>-1\"\n",
      "}\n",
      "\n",
      "# Create<|mask|> security group to allow traffic to the ALB\n",
      "<|mask|><|mask|>aws_security_group\" \"alb<|mask|>\" {\n",
      "<|mask|> name<|mask|> =<|mask|>alb_sg<|mask|> <|mask|> {\n",
      "<|mask|> from_port =<|mask|>80\n",
      "    to<|mask|> = <|mask|>0\n",
      "<|mask|><|mask|> =<|mask|>tcp<|mask|>   <|mask|>r_blocks = [\"0.0.0.0<|mask|>0\"]\n",
      "<|mask|><|mask|>}\n",
      "\n",
      "<|mask|> Create<|mask|><|mask|><|mask|> and<|mask|> group\n",
      "resource<|mask|>aws<|mask|>\"<|mask|>alb\"<|mask|><|mask|> name              <|mask|> \"example-al<|mask|><|mask|> <|mask|>           = false<|mask|><|mask|> load_balancer_type = \"application\"\n",
      "\n",
      " <|mask|>nets<|mask|> [\"<|mask|>-1<|mask|>345678\", \"subnet<|mask|>8765<|mask|>321\"]\n",
      "\n",
      "  security<|mask|><|mask|> [<|mask|>_security_group.alb_sg<|mask|>]\n",
      "\n",
      " <|mask|><|mask|> {\n",
      "   <|mask|><|mask|> \"<|mask|><|mask|><|mask|> }\n",
      "<|mask|>resource \"aws_lb<|mask|>_group<|mask|> \"target<|mask|>\" {\n",
      " <|mask|>        =<|mask|>example-target-group\"\n",
      "  port<|mask|><|mask|><|mask|>80\n",
      "<|mask|> protocol    = \"<|mask|>\"\n",
      "  vpc<|mask|><|mask|> = \"vpc-12<|mask|>456<|mask|>8\"\n",
      "\n",
      " <|mask|>_check {\n",
      "    path = \"/health\"\n",
      "<|mask|> }\n",
      "\n",
      "  tags = {\n",
      "   <|mask|><|mask|><|mask|>production\"\n",
      "  }\n",
      "<|mask|>#<|mask|> an autoscaling<|mask|>\n",
      "<|mask|> \"aws_launch_configuration<|mask|> \"launch_configuration\" {\n",
      "<|mask|> name_prefix   =<|mask|><|mask|><|mask|>c-<|mask|> <|mask|>_id      =<|mask|>ami-12<|mask|>4<|mask|>6<|mask|><|mask|>\"\n",
      "<|mask|><|mask|>_type = \"t2.micro\"\n",
      "\n",
      "  # Other settings here (e<|mask|>. security<|mask|>, userdata, etc.)\n",
      "}\n",
      "\n",
      "resource \"aws_autoscaling<|mask|>\" \"<|mask|>caling<|mask|><|mask|> {\n",
      "  name                      =<|mask|>example-asg\"\n",
      "  launch_configuration      = aws<|mask|>_configuration.launch_configuration<|mask|>\n",
      "<|mask|> target_group<|mask|><|mask|>s         = [<|mask|>_lb<|mask|>_group.target<|mask|><|mask|><|mask|>]\n",
      "  health_check_type<|mask|> =<|mask|>EC2\"\n",
      " <|mask|>_size                  =<|mask|>1\n",
      " <|mask|>_size                  = 3\n",
      " <|mask|>_capacity          = 2<|mask|>  vpc_zone<|mask|><|mask|><|mask|> [\"subnet-12345678\", \"<|mask|><|mask|>8765<|mask|>321\"]\n",
      "<|mask|> termination_policies     <|mask|> [\"Default\"]\n",
      "<|mask|> wait_for_capacity_timeout = \"1<|mask|>m<|mask|>  tags<|mask|> {\n",
      "    Environment = \"<|mask|>\"\n",
      "  }\n",
      "<|mask|>```<|mask|>Note that you will<|mask|> to<|mask|> the<|mask|> for your specific use<|mask|> (e.g. specify your<|mask|><|mask|>I ID, VPC ID, subnet IDs, etc<|mask|> This<|mask|> just<|mask|> example to<|mask|><|mask|><|mask|> idea<|mask|> how<|mask|><|mask|> an autos<|mask|><|mask|> with<|mask|> AL<|mask|> in Terraform<|mask|><|mask|>\n",
      "\n",
      "====================================================================================================\n",
      "Sure, here's an example Terraform HCL code that creates an AWS Autoscaling Group and an Application Load Balancer to expose an application to the internet:\n",
      "``` \n",
      "# Configure the AWS provider\n",
      "provider \"aws\" {\n",
      "  region = \"us-east-1\"\n",
      "}\n",
      "\n",
      "# Create a security group to allow traffic to the ALB\n",
      "resource \"aws_security_group\" \"alb_sg\" {\n",
      "  name_prefix = \"alb_sg\"\n",
      "  ingress {\n",
      "    from_port = 80\n",
      "    to_port = 80\n",
      "    protocol = \"tcp\"\n",
      "    cidr_blocks = [\"0.0.0.0/0\"]\n",
      "  }\n",
      "}\n",
      "\n",
      "# Create an ALB and target group\n",
      "resource \"aws_lb\" \"alb\" {\n",
      "  name               = \"example-alb\"\n",
      "  internal           = false\n",
      "  load_balancer_type = \"application\"\n",
      "\n",
      "  subnets = [\"subnet-12345678\", \"subnet-87654321\"]\n",
      "\n",
      "  security_groups = [aws_security_group.alb_sg.id]\n",
      "\n",
      "  tags = {\n",
      "    Environment = \"production\"\n",
      "  }\n",
      "}\n",
      "\n",
      "resource \"aws_lb_target_group\" \"target_group\" {\n",
      "  name        = \"example-target-group\"\n",
      "  port        = 80\n",
      "  protocol    = \"HTTP\"\n",
      "  vpc_id      = \"vpc-12345678\"\n",
      "\n",
      "  health_check {\n",
      "    path = \"/health\"\n",
      "  }\n",
      "\n",
      "  tags = {\n",
      "    Environment = \"production\"\n",
      "  }\n",
      "}\n",
      "\n",
      "# Create an autoscaling group\n",
      "resource \"aws_launch_configuration\" \"launch_configuration\" {\n",
      "  name_prefix   = \"example-lc-\"\n",
      "  image_id      = \"ami-12345678\"\n",
      "  instance_type = \"t2.micro\"\n",
      "\n",
      "  # Other settings here (e.g. security groups, userdata, etc.)\n",
      "}\n",
      "\n",
      "resource \"aws_autoscaling_group\" \"autoscaling_group\" {\n",
      "  name                      = \"example-asg\"\n",
      "  launch_configuration      = aws_launch_configuration.launch_configuration.id\n",
      "  target_group_arns         = [aws_lb_target_group.target_group.arn]\n",
      "  health_check_type         = \"EC2\"\n",
      "  min_size                  = 1\n",
      "  max_size                  = 3\n",
      "  desired_capacity          = 2\n",
      "  vpc_zone_identifier       = [\"subnet-12345678\", \"subnet-87654321\"]\n",
      "  termination_policies      = [\"Default\"]\n",
      "  wait_for_capacity_timeout = \"10m\"\n",
      "\n",
      "  tags = {\n",
      "    Environment = \"production\"\n",
      "  }\n",
      "}\n",
      "``` \n",
      "Note that you will need to customize the settings for your specific use case (e.g. specify your own AMI ID, VPC ID, subnet IDs, etc.). This is just an example to give you an idea of how to create an autoscaling group with an ALB in Terraform.\n",
      "<|im_start|>user\n",
      "¿Por qué crees que cada año es más difícil tener una casa propia en comparación a décadas anteriores?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Existen<|mask|><|mask|>ores<|mask|> pueden<|mask|>uir a que cada<|mask|> sea<|mask|><|mask|> tener una casa propia en comparación con<|mask|>adas anteriores. Algunos de los fact<|mask|><|mask|><|mask|> son<|mask|><|mask|> El aumento del precio de la vivienda: En<|mask|><|mask|><|mask|><|mask|> mundo<|mask|> el<|mask|> de la<|mask|>ienda<|mask|> aument<|mask|><|mask|>mente en las últimas décadas,<|mask|> que hace<|mask|> sea más difícil para muchas<|mask|> comprar una casa propia<|mask|> Esto se<|mask|> a una combinación de fact<|mask|><|mask|> incluyendo la esc<|mask|><|mask|> de viviendas disponibles, el aumento<|mask|><|mask|> demanda de viviendas<|mask|><|mask|> el aumento del costo de la<|mask|>.\n",
      "<|mask|> La falta de<|mask|> a créd<|mask|><|mask|><|mask|> algunos<|mask|>, las personas pueden<|mask|> dificultades para obtener un crédito hip<|mask|>ario debido<|mask|> la falta<|mask|> histor<|mask|> crediticio<|mask|> ingresos est<|mask|>. Esto hace que sea<|mask|> difícil<|mask|> muchas personas<|mask|> una casa<|mask|>.\n",
      "- El aumento de<|mask|> deuda estudiantil: Much<|mask|><|mask|><|mask|> buscan comprar una casa<|mask|> también enfrent<|mask|> una gran de<|mask|><|mask|><|mask|>il<|mask|> lo que puede dificultar el ahor<|mask|> para<|mask|> pago<|mask|> o el cumplimiento<|mask|><|mask|> requis<|mask|><|mask|> los<|mask|>amistas hipotecarios.\n",
      "- Cambios dem<|mask|>icos<|mask|> Las tend<|mask|> demográficas también pueden<|mask|>ar el<|mask|> de la vivienda. Por ejemplo,<|mask|> aumento de las tasas<|mask|> divorcio ha dado lugar a más hog<|mask|> uniperson<|mask|>, lo<|mask|> puede dific<|mask|>ar a<|mask|> personas pagar una hipoteca por<|mask|> cuenta.<|im_end|>\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "====================================================================================================\n",
      "Existen varios factores que pueden contribuir a que cada año sea más difícil tener una casa propia en comparación con décadas anteriores. Algunos de los factores más importantes son:\n",
      "- El aumento del precio de la vivienda: En muchas ciudades del mundo, el precio de la vivienda ha aumentado considerablemente en las últimas décadas, lo que hace que sea más difícil para muchas personas comprar una casa propia. Esto se debe a una combinación de factores, incluyendo la escasez de viviendas disponibles, el aumento de la demanda de viviendas, y el aumento del costo de la construcción.\n",
      "- La falta de acceso a créditos: En algunos casos, las personas pueden tener dificultades para obtener un crédito hipotecario debido a la falta de historial crediticio o ingresos estables. Esto hace que sea más difícil para muchas personas comprar una casa propia.\n",
      "- El aumento de la deuda estudiantil: Muchos jóvenes que buscan comprar una casa propia también enfrentan una gran deuda estudiantil, lo que puede dificultar el ahorro para un pago inicial o el cumplimiento de los requisitos de los prestamistas hipotecarios.\n",
      "- Cambios demográficos: Las tendencias demográficas también pueden afectar el mercado de la vivienda. Por ejemplo, el aumento de las tasas de divorcio ha dado lugar a más hogares unipersonales, lo que puede dificultar a algunas personas pagar una hipoteca por su cuenta.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sachinthaka/work/canva/tools/build/python/third_party/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sample_messages = [raw_ds[0], raw_ds[1]]\n",
    "\n",
    "collate_fn = SFTCollateFn(tokenizer)\n",
    "batch = collate_fn(sample_messages)\n",
    "\n",
    "for i in range(len(batch[\"input_ids\"])):\n",
    "    line = batch[\"input_ids\"][i]\n",
    "    output = tokenizer.decode(line[line != -100], skip_special_tokens=False)\n",
    "    print(output)\n",
    "    print(\"=\" * 100)\n",
    "    expected_output = [\n",
    "        message[\"content\"] for message in raw_ds[i][\"messages\"] if message[\"role\"] == \"assistant\"\n",
    "    ][0]\n",
    "    print(expected_output.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed287b",
   "metadata": {},
   "source": [
    "### Setup LoRA for Efficient Fine-tuning\n",
    "\n",
    "LoRA (Low-Rank Adaptation) allows us to fine-tune large models efficiently by only training small adapter matrices. This significantly reduces memory usage and training time while maintaining good performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "21935bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,030,144 || all params: 4,054,817,280 || trainable%: 0.8146\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "lora_config = peft.LoraConfig(**trainer_config.lora_config.model_dump())\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_model = peft.get_peft_model(llm_model, lora_config)\n",
    "lora_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def _calculate_accuracy(\n",
    "    model: transformers.PreTrainedModel, batch: dict[str, torch.Tensor], mask_token_id: int\n",
    ") -> tuple[float, float]:\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    mask_token_id_mask = input_ids == mask_token_id\n",
    "    non_mask_token_id_mask = (~mask_token_id_mask) & (attention_mask == 1)\n",
    "\n",
    "    non_mask_accuracy = (\n",
    "        (predicted_ids[non_mask_token_id_mask] == labels[non_mask_token_id_mask]).mean().item()\n",
    "    )\n",
    "    mask_accuracy = (predicted_ids[mask_token_id_mask] == labels[mask_token_id_mask]).mean().item()\n",
    "    return non_mask_accuracy, mask_accuracy\n",
    "\n",
    "\n",
    "class AccuracyCallback(TrainerCallback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: transformers.PreTrainedModel,\n",
    "        dataset: datasets.Dataset,\n",
    "        log_interval: int,\n",
    "        collate_fn: CollateFn,\n",
    "        batch_size: int,\n",
    "    ):\n",
    "        self.log_interval = log_interval\n",
    "        self.collate_fn = collate_fn\n",
    "        self.dataset = dataset\n",
    "        self.last_logged_step = 0\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        # Log predictions every log_interval steps\n",
    "        if (\n",
    "            state.global_step > 0\n",
    "            and state.global_step % self.log_interval == 0\n",
    "            and state.global_step != self.last_logged_step\n",
    "        ):\n",
    "            sample_data = [\n",
    "                self.dataset[i % len(self.dataset)]\n",
    "                for i in range(state.global_step, state.global_step + self.batch_size)\n",
    "            ]\n",
    "            batch = self.collate_fn(sample_data)\n",
    "            self.model.eval()\n",
    "            overall_accuracy, mask_accuracy = _calculate_accuracy(\n",
    "                self.model, batch, self.collate_fn.mask_token_id\n",
    "            )\n",
    "            self.model.train()\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"overall_accuracy\": overall_accuracy,\n",
    "                    \"mask_accuracy\": mask_accuracy,\n",
    "                    \"step\": state.global_step,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9058082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_idx(row_ids: torch.Tensor, end_token_id: int, max_length: int) -> int:\n",
    "    pos = (row_ids == end_token_id).nonzero(as_tuple=False)\n",
    "    return pos[0].item() if pos.numel() > 0 else max_length\n",
    "\n",
    "\n",
    "def get_quotas(init_mask_counts: torch.Tensor, steps: int) -> torch.Tensor:\n",
    "    base = init_mask_counts // steps\n",
    "    rem = init_mask_counts % steps\n",
    "    return torch.stack(\n",
    "        [(base + (rem > s).long()) for s in range(steps)],\n",
    "        dim=0,\n",
    "    )\n",
    "\n",
    "\n",
    "class DiffusionInference:\n",
    "    \"\"\"\n",
    "    Iteratively infills mask tokens over `steps`. For each batch row:\n",
    "      - Compute the initial number of masks *before* the first EOS (if any).\n",
    "      - Divide that count across steps (distributing remainders to earlier steps).\n",
    "      - At each step, choose the top-k masked positions by confidence (max logit)\n",
    "        and fill only those with their argmax token.\n",
    "      - On the final step, unmask all remaining (before EOS).\n",
    "    Positions after the first EOS (pre-existing or newly predicted) are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: transformers.PreTrainedModel,\n",
    "        mask_token_id: int,\n",
    "        end_token_id: int,\n",
    "        steps: int,\n",
    "    ):\n",
    "        assert steps >= 1, \"steps must be >= 1\"\n",
    "        self.model = model\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.end_token_id = end_token_id\n",
    "        self.steps = steps\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, batch: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
    "        was_training = self.model.training\n",
    "        self.model.eval()\n",
    "\n",
    "        # Work on copies to avoid mutating the original batch in-place.\n",
    "        input_ids = batch[\"input_ids\"].clone()\n",
    "        attention_mask = batch.get(\"attention_mask\", None)\n",
    "\n",
    "        device = input_ids.device\n",
    "        batch_size, L = input_ids.shape\n",
    "\n",
    "        # Initial EOS boundaries and initial mask counts (only up to EOS!)\n",
    "        eos_idx = torch.tensor(\n",
    "            [first_idx(input_ids[b], self.end_token_id, L) for b in range(B)]\n",
    "        ).to(device)\n",
    "        init_mask_counts = torch.tensor(\n",
    "            [(input_ids[b, : eos_idx[b]] == self.mask_token_id).sum() for b in range(B)]\n",
    "        ).to(device)\n",
    "\n",
    "        quotas = get_quotas(init_mask_counts, self.steps)\n",
    "\n",
    "        for s in range(self.steps):\n",
    "            # Forward pass\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits  # [B, L, V]\n",
    "\n",
    "            # For each row, pick top-k masked positions by confidence (max logit).\n",
    "            for b in range(batch_size):\n",
    "                # Recompute EOS boundary each step (it may have been newly predicted).\n",
    "                e = first_idx(input_ids[b], self.end_token_id, L)\n",
    "\n",
    "                # Masked positions strictly before EOS\n",
    "                mask_pos = (\n",
    "                    (input_ids[b, :e] == self.mask_token_id).nonzero(as_tuple=False).squeeze(-1)\n",
    "                )\n",
    "                if mask_pos.numel() == 0:\n",
    "                    continue\n",
    "\n",
    "                k = int(min(quotas[s, b].item(), mask_pos.numel()))\n",
    "\n",
    "                # Compute per-position confidence and predicted token.\n",
    "                # conf: [num_masks], pred_tok: [num_masks]\n",
    "                per_pos_logits = logits[b, mask_pos, :]  # [num_masks, V]\n",
    "                confidence, pred_tok = per_pos_logits.max(\n",
    "                    dim=-1\n",
    "                )  # argmax token + its logit, size: [num_masks]\n",
    "\n",
    "                # Select top-k positions by confidence.\n",
    "                topk_idx = confidence.topk(k=k, dim=0).indices  # [k]\n",
    "                pos_to_update = mask_pos[topk_idx]  # [k]\n",
    "                # Overwrite selected masked positions with their predicted tokens.\n",
    "                input_ids[b, pos_to_update] = pred_tok[topk_idx]  # [k]\n",
    "\n",
    "            # Optional early exit if nothing masked remains anywhere.\n",
    "            if not (input_ids == self.mask_token_id).any():\n",
    "                break\n",
    "\n",
    "        if was_training:\n",
    "            self.model.train()\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to log predictions every N steps\n",
    "class PredictionLoggingCallback(TrainerCallback):\n",
    "    def __init__(self, log_interval: int, collate_fn, dataset):\n",
    "        self.log_interval = log_interval\n",
    "        self.collate_fn = collate_fn\n",
    "        self.dataset = dataset\n",
    "        self.last_logged_step = 0\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        # Log predictions every log_interval steps\n",
    "        if state.global_step > 0 and state.global_step % self.log_interval == 0 and state.global_step != self.last_logged_step:\n",
    "            model = kwargs.get(\"model\")\n",
    "            # Create a small sample batch from the dataset\n",
    "            sample_data = []\n",
    "            for i, example in enumerate(self.dataset):\n",
    "                if i >= 16:\n",
    "                    break\n",
    "                sample_data.append(example)\n",
    "            batch = self.collate_fn(sample_data)\n",
    "            log_predictions(model, batch, state.global_step)\n",
    "            self.last_logged_step = state.global_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced78112",
   "metadata": {},
   "source": [
    "### Training with HuggingFace Trainer\n",
    "\n",
    "Using the standard `Trainer` (not `SFTTrainer`) to preserve full sequence training:\n",
    "- `SFTTrainer` would automatically mask non-assistant tokens for instruction fine-tuning\n",
    "- We use `Trainer` with our custom `CollateFn` to train on the **entire sequence**\n",
    "- Uses `trainer_config.gradient_accumulation_steps` from config\n",
    "- Custom callback logs 8 predictions every 1000 iterations to wandb tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7cae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training arguments using values from trainer_config\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_adapter\",\n",
    "    num_train_epochs=trainer_config.epochs,\n",
    "    per_device_train_batch_size=trainer_config.batch_size,\n",
    "    gradient_accumulation_steps=trainer_config.gradient_accumulation_steps,\n",
    "    learning_rate=trainer_config.lr,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False,\n",
    "    bf16=device.type in {\"cuda\", \"mps\"},\n",
    ")\n",
    "\n",
    "# Create the Trainer (NOT SFTTrainer to preserve full sequence training)\n",
    "# SFTTrainer would mask non-assistant tokens, but we want to train on the entire sequence\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=raw_ds,\n",
    "    data_collator=collate_fn,\n",
    "    callbacks=[PredictionLoggingCallback(trainer_config.log_interval, collate_fn, raw_ds)],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
